{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f75e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0827e",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87946d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fe39",
   "metadata": {},
   "source": [
    "DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e1d0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "base_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "data_dir = base_dir / \"src\" / \"data\" / \"raw\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"train.csv\")\n",
    "test = pd.read_csv(data_dir / \"test.csv\")\n",
    "test_labels = pd.read_csv(data_dir / \"test_labels.csv\") \n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b7e08",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a477f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "def prepare_test_for_evaluation(test_df, test_labels_df):\n",
    "    valid_mask = (test_labels_df[labels] != -1).all(axis=1)\n",
    "    print(f\"Test samples: {len(test_df)} total, {valid_mask.sum()} valid for evaluation\")\n",
    "    return test_df[valid_mask].copy(), test_labels_df[valid_mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8b73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_subset(df, labels, n_per_class=200):\n",
    "    \"\"\"\n",
    "    Crea un subset balanceado para evaluación más justa\n",
    "    \"\"\"\n",
    "    idxs = set()\n",
    "    print(f\"Creating balanced subset with {n_per_class} samples per class...\")\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # Encuentra índices donde esa clase está presente\n",
    "        class_idxs = np.where(df[label].values == 1)[0]\n",
    "        available = len(class_idxs)\n",
    "        to_sample = min(n_per_class, available)\n",
    "        \n",
    "        if available > 0:\n",
    "            chosen = np.random.choice(class_idxs, to_sample, replace=False)\n",
    "            idxs.update(chosen)\n",
    "            print(f\"  {label}: {to_sample}/{available} samples\")\n",
    "        else:\n",
    "            print(f\"  {label}: 0 samples available!\")\n",
    "    \n",
    "    # Añade algunos neutros (all zero)\n",
    "    neutral_idxs = np.where(df[labels].sum(axis=1) == 0)[0]\n",
    "    neutral_available = len(neutral_idxs)\n",
    "    neutral_to_sample = min(n_per_class, neutral_available)\n",
    "    \n",
    "    if neutral_available > 0:\n",
    "        chosen_neutral = np.random.choice(neutral_idxs, neutral_to_sample, replace=False)\n",
    "        idxs.update(chosen_neutral)\n",
    "        print(f\"  neutral: {neutral_to_sample}/{neutral_available} samples\")\n",
    "    \n",
    "    # Devuelve el subset balanceado\n",
    "    idxs = list(idxs)\n",
    "    balanced_df = df.iloc[idxs].copy()\n",
    "    print(f\"Total balanced samples: {len(balanced_df)}\")\n",
    "    return balanced_df\n",
    "\n",
    "def calculate_class_weights(train_df, labels):\n",
    "    \"\"\"\n",
    "    Calcula pesos para manejar el desbalanceo de clases\n",
    "    \"\"\"\n",
    "    class_counts = train_df[labels].sum(axis=0)\n",
    "    total = len(train_df)\n",
    "    pos_weights = (total - class_counts) / (class_counts + 1e-6)\n",
    "    \n",
    "    print(\"Class distribution and weights:\")\n",
    "    for label, count, weight in zip(labels, class_counts, pos_weights):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {label}: {count} samples ({percentage:.2f}%) -> weight: {weight:.2f}\")\n",
    "    \n",
    "    return pos_weights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894b8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_neutral_performance(y_true, y_pred_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evalúa específicamente el rendimiento en comentarios neutros\n",
    "    \"\"\"\n",
    "    # Convertir probabilidades a predicciones binarias\n",
    "    y_pred_bin = (y_pred_probs > threshold).astype(int)\n",
    "    \n",
    "    # Identificar comentarios neutros (todas las etiquetas = 0)\n",
    "    neutral_mask = (y_true.sum(axis=1) == 0)\n",
    "    neutral_total = neutral_mask.sum()\n",
    "    \n",
    "    if neutral_total == 0:\n",
    "        return {\n",
    "            'neutral_total': 0,\n",
    "            'neutral_correct': 0,\n",
    "            'neutral_accuracy': 0.0,\n",
    "            'neutral_fp_rate': 0.0\n",
    "        }\n",
    "    \n",
    "    # ¿Cuántos neutros fueron predichos como neutros?\n",
    "    neutral_pred_mask = (y_pred_bin[neutral_mask].sum(axis=1) == 0)\n",
    "    neutral_correct = neutral_pred_mask.sum()\n",
    "    neutral_fp = neutral_total - neutral_correct\n",
    "    \n",
    "    # Calcular métricas\n",
    "    neutral_accuracy = neutral_correct / neutral_total\n",
    "    neutral_fp_rate = neutral_fp / neutral_total\n",
    "    \n",
    "    return {\n",
    "        'neutral_total': int(neutral_total),\n",
    "        'neutral_correct': int(neutral_correct),\n",
    "        'neutral_fp': int(neutral_fp),\n",
    "        'neutral_accuracy': float(neutral_accuracy),\n",
    "        'neutral_fp_rate': float(neutral_fp_rate)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "639a41d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 153164 total, 63978 valid for evaluation\n",
      "Train size: 143613\n",
      "Validation size: 15958\n",
      "Test evaluation size: 63978\n",
      "Class distribution and weights:\n",
      "  toxic: 15294 samples (9.58%) -> weight: 9.43\n",
      "  severe_toxic: 1595 samples (1.00%) -> weight: 99.04\n",
      "  obscene: 8449 samples (5.29%) -> weight: 17.89\n",
      "  threat: 478 samples (0.30%) -> weight: 332.83\n",
      "  insult: 7877 samples (4.94%) -> weight: 19.26\n",
      "  identity_hate: 1405 samples (0.88%) -> weight: 112.57\n",
      "Creating balanced validation set\n",
      "Creating balanced subset with 300 samples per class...\n",
      "  toxic: 300/1529 samples\n",
      "  severe_toxic: 149/149 samples\n",
      "  obscene: 300/847 samples\n",
      "  threat: 50/50 samples\n",
      "  insult: 300/800 samples\n",
      "  identity_hate: 153/153 samples\n",
      "  neutral: 300/14355 samples\n",
      "Total balanced samples: 1153\n",
      "Creating balanced test set\n",
      "Creating balanced subset with 200 samples per class...\n",
      "  toxic: 200/6090 samples\n",
      "  severe_toxic: 200/367 samples\n",
      "  obscene: 200/3691 samples\n",
      "  threat: 200/211 samples\n",
      "  insult: 200/3427 samples\n",
      "  identity_hate: 200/712 samples\n",
      "  neutral: 200/57735 samples\n",
      "Total balanced samples: 1259\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "test_eval, test_labels_eval = prepare_test_for_evaluation(test, test_labels)\n",
    "\n",
    "\n",
    "# Split training data\n",
    "X = train['comment_text'].values\n",
    "y = train[labels].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y[:, 0]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test evaluation size: {len(test_eval)}\")\n",
    "\n",
    "# Calcular pesos para el desbalanceo de clases\n",
    "class_weights = calculate_class_weights(train, labels)\n",
    "pos_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Crear datasets balanceados para evaluación\n",
    "print(\"Creating balanced validation set\")\n",
    "val_df_temp = pd.DataFrame({'comment_text': X_val})\n",
    "val_df_temp[labels] = y_val\n",
    "balanced_val_df = create_balanced_subset(val_df_temp, labels, n_per_class=300)\n",
    "\n",
    "print(\"Creating balanced test set\")\n",
    "test_df_temp = test_eval.copy()\n",
    "test_df_temp[labels] = test_labels_eval[labels].values\n",
    "balanced_test_df = create_balanced_subset(test_df_temp, labels, n_per_class=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e24f1",
   "metadata": {},
   "source": [
    "MODELS TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ba0fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "MODELS_TO_TEST = [\n",
    "    {\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'batch_size': 32,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'roberta-base',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'microsoft/deberta-v3-base',\n",
    "        'batch_size': 8,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'unitary/toxic-bert',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb69ca",
   "metadata": {},
   "source": [
    "EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f26e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, eval_df, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on balanced validation set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        X_eval = eval_df['comment_text'].values\n",
    "        y_eval = eval_df[labels].values\n",
    "        \n",
    "        for j in tqdm(range(0, len(X_eval), batch_size), desc=f\"Evaluating {model_name}\"):\n",
    "            batch_texts = X_eval[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        # Calcular AUC por etiqueta\n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if len(np.unique(y_eval[:, i])) > 1:  # Solo si hay ambas clases\n",
    "                auc = roc_auc_score(y_eval[:, i], predictions[:, i])\n",
    "                label_aucs.append(auc)\n",
    "                print(f\"  {label}: {auc:.4f}\")\n",
    "            else:\n",
    "                label_aucs.append(0.0)\n",
    "                print(f\"  {label}: No samples or single class\")\n",
    "        \n",
    "        mean_auc = np.mean([auc for auc in label_aucs if auc > 0])\n",
    "        print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        # Evaluar rendimiento en neutros\n",
    "        neutral_metrics = evaluate_neutral_performance(y_eval, predictions)\n",
    "        print(f\"  Neutral accuracy: {neutral_metrics['neutral_accuracy']:.4f}\")\n",
    "        print(f\"  Neutral FP rate: {neutral_metrics['neutral_fp_rate']:.4f}\")\n",
    "        print(f\"  Neutral samples: {neutral_metrics['neutral_total']}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs, neutral_metrics, predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels), {}, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b1eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model_name, test_eval_df, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on balanced test set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        X_test = test_eval_df['comment_text'].values\n",
    "        y_test = test_eval_df[labels].values\n",
    "        \n",
    "        for j in tqdm(range(0, len(X_test), batch_size), desc=f\"Test evaluation\"):\n",
    "            batch_texts = X_test[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        # Calcular AUC por etiqueta\n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if len(np.unique(y_test[:, i])) > 1:\n",
    "                auc = roc_auc_score(y_test[:, i], predictions[:, i])\n",
    "                label_aucs.append(auc)\n",
    "                print(f\"  {label}: {auc:.4f}\")\n",
    "            else:\n",
    "                label_aucs.append(0.0)\n",
    "                print(f\"  {label}: No samples or single class\")\n",
    "        \n",
    "        mean_auc = np.mean([auc for auc in label_aucs if auc > 0])\n",
    "        print(f\"  Test Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        # Evaluar rendimiento en neutros\n",
    "        neutral_metrics = evaluate_neutral_performance(y_test, predictions)\n",
    "        print(f\"  Test Neutral accuracy: {neutral_metrics['neutral_accuracy']:.4f}\")\n",
    "        print(f\"  Test Neutral FP rate: {neutral_metrics['neutral_fp_rate']:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs, neutral_metrics, predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels), {}, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e7957",
   "metadata": {},
   "source": [
    "TEST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc22028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/179631418936248154', creation_time=1754475070403, experiment_id='179631418936248154', last_update_time=1754475070403, lifecycle_stage='active', name='HuggingFace_Baselines_balanced', tags={}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"HuggingFace_Baselines_balanced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ace9998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased on balanced validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "W0806 12:17:34.329000 14872 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating distilbert-base-uncased: 100%|██████████| 73/73 [00:07<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.5924\n",
      "  severe_toxic: 0.5704\n",
      "  obscene: 0.6236\n",
      "  threat: 0.5844\n",
      "  insult: 0.5519\n",
      "  identity_hate: 0.4113\n",
      "  Mean AUC: 0.5557\n",
      "  Neutral accuracy: 0.0733\n",
      "  Neutral FP rate: 0.9267\n",
      "  Neutral samples: 300\n",
      "Evaluating distilbert-base-uncased on balanced test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 79/79 [00:06<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4377\n",
      "  severe_toxic: 0.4613\n",
      "  obscene: 0.4438\n",
      "  threat: 0.5070\n",
      "  insult: 0.4269\n",
      "  identity_hate: 0.4768\n",
      "  Test Mean AUC: 0.4589\n",
      "  Test Neutral accuracy: 0.0000\n",
      "  Test Neutral FP rate: 1.0000\n",
      "Evaluating bert-base-uncased on balanced validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating bert-base-uncased: 100%|██████████| 73/73 [00:12<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.6282\n",
      "  severe_toxic: 0.4156\n",
      "  obscene: 0.5628\n",
      "  threat: 0.5102\n",
      "  insult: 0.3966\n",
      "  identity_hate: 0.4030\n",
      "  Mean AUC: 0.4861\n",
      "  Neutral accuracy: 0.0000\n",
      "  Neutral FP rate: 1.0000\n",
      "  Neutral samples: 300\n",
      "Evaluating bert-base-uncased on balanced test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 79/79 [00:13<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4013\n",
      "  severe_toxic: 0.5330\n",
      "  obscene: 0.3676\n",
      "  threat: 0.4758\n",
      "  insult: 0.3992\n",
      "  identity_hate: 0.5768\n",
      "  Test Mean AUC: 0.4590\n",
      "  Test Neutral accuracy: 0.0000\n",
      "  Test Neutral FP rate: 1.0000\n",
      "Evaluating roberta-base on balanced validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating roberta-base: 100%|██████████| 73/73 [00:12<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4107\n",
      "  severe_toxic: 0.4396\n",
      "  obscene: 0.5500\n",
      "  threat: 0.4592\n",
      "  insult: 0.5205\n",
      "  identity_hate: 0.6310\n",
      "  Mean AUC: 0.5018\n",
      "  Neutral accuracy: 0.0000\n",
      "  Neutral FP rate: 1.0000\n",
      "  Neutral samples: 300\n",
      "Evaluating roberta-base on balanced test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 79/79 [00:13<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.6612\n",
      "  severe_toxic: 0.5367\n",
      "  obscene: 0.5758\n",
      "  threat: 0.4825\n",
      "  insult: 0.4510\n",
      "  identity_hate: 0.5201\n",
      "  Test Mean AUC: 0.5379\n",
      "  Test Neutral accuracy: 0.0000\n",
      "  Test Neutral FP rate: 1.0000\n",
      "Evaluating microsoft/deberta-v3-base on balanced validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 371M/371M [00:26<00:00, 14.2MB/s] \n",
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\analyst4\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating microsoft/deberta-v3-base: 100%|██████████| 73/73 [00:18<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.5487\n",
      "  severe_toxic: 0.5608\n",
      "  obscene: 0.3897\n",
      "  threat: 0.5005\n",
      "  insult: 0.5794\n",
      "  identity_hate: 0.4684\n",
      "  Mean AUC: 0.5079\n",
      "  Neutral accuracy: 0.0000\n",
      "  Neutral FP rate: 1.0000\n",
      "  Neutral samples: 300\n",
      "Evaluating microsoft/deberta-v3-base on balanced test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4189\n",
      "  severe_toxic: 0.5330\n",
      "  obscene: 0.4278\n",
      "  threat: 0.4793\n",
      "  insult: 0.5496\n",
      "  identity_hate: 0.5291\n",
      "  Test Mean AUC: 0.4896\n",
      "  Test Neutral accuracy: 0.0000\n",
      "  Test Neutral FP rate: 1.0000\n",
      "Evaluating unitary/toxic-bert on balanced validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating unitary/toxic-bert: 100%|██████████| 73/73 [00:12<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.9835\n",
      "  severe_toxic: 0.9045\n",
      "  obscene: 0.9819\n",
      "  threat: 0.9728\n",
      "  insult: 0.9558\n",
      "  identity_hate: 0.9755\n",
      "  Mean AUC: 0.9623\n",
      "  Neutral accuracy: 1.0000\n",
      "  Neutral FP rate: 0.0000\n",
      "  Neutral samples: 300\n",
      "Evaluating unitary/toxic-bert on balanced test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test evaluation: 100%|██████████| 79/79 [00:13<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.9669\n",
      "  severe_toxic: 0.8725\n",
      "  obscene: 0.9484\n",
      "  threat: 0.9730\n",
      "  insult: 0.9085\n",
      "  identity_hate: 0.9570\n",
      "  Test Mean AUC: 0.9377\n",
      "  Test Neutral accuracy: 0.9050\n",
      "  Test Neutral FP rate: 0.0950\n",
      "\n",
      "================================================================================\n",
      "BALANCED EVALUATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "Model                          Val AUC    Test AUC   Val Neutral Acc Test Neutral Acc\n",
      "--------------------------------------------------------------------------------\n",
      "unitary/toxic-bert             0.9623     0.9377     1.0000          0.9050\n",
      "distilbert-base-uncased        0.5557     0.4589     0.0733          0.0000\n",
      "microsoft/deberta-v3-base      0.5079     0.4896     0.0000          0.0000\n",
      "roberta-base                   0.5018     0.5379     0.0000          0.0000\n",
      "bert-base-uncased              0.4861     0.4590     0.0000          0.0000\n",
      "\n",
      "================================================================================\n",
      "NEUTRAL PERFORMANCE DETAILS\n",
      "================================================================================\n",
      "\n",
      "distilbert-base-uncased:\n",
      "  Validation - Total neutral: 300, correct: 22, FP Rate: 0.9267\n",
      "  Test - Total neutral: 200, correct: 0, FP Rate: 1.0000\n",
      "\n",
      "bert-base-uncased:\n",
      "  Validation - Total neutral: 300, correct: 0, FP Rate: 1.0000\n",
      "  Test - Total neutral: 200, correct: 0, FP Rate: 1.0000\n",
      "\n",
      "roberta-base:\n",
      "  Validation - Total neutral: 300, correct: 0, FP Rate: 1.0000\n",
      "  Test - Total neutral: 200, correct: 0, FP Rate: 1.0000\n",
      "\n",
      "microsoft/deberta-v3-base:\n",
      "  Validation - Total neutral: 300, correct: 0, FP Rate: 1.0000\n",
      "  Test - Total neutral: 200, correct: 0, FP Rate: 1.0000\n",
      "\n",
      "unitary/toxic-bert:\n",
      "  Validation - Total neutral: 300, correct: 300, FP Rate: 0.0000\n",
      "  Test - Total neutral: 200, correct: 181, FP Rate: 0.0950\n",
      "Class weights used: {'toxic': 9.433568719142567, 'severe_toxic': 99.04451404448619, 'obscene': 17.886377083928704, 'threat': 332.83054323675617, 'insult': 19.257839276468474, 'identity_hate': 112.57366540030343}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test each model\n",
    "results = {}\n",
    "\n",
    "for model_config in MODELS_TO_TEST:\n",
    "    with mlflow.start_run(run_name=f\"balanced_{model_config['name'].split('/')[-1]}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(model_config)\n",
    "        mlflow.log_param('class_weights_used', True)\n",
    "        mlflow.log_param('balanced_evaluation', True)\n",
    "        \n",
    "        # Evaluate on balanced validation set\n",
    "        val_auc, val_label_aucs, val_neutral_metrics, _ = evaluate_model(\n",
    "            model_config['name'],\n",
    "            balanced_val_df,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # Evaluate on balanced test set\n",
    "        test_auc, test_label_aucs, test_neutral_metrics, _ = evaluate_model_on_test(\n",
    "            model_config['name'],\n",
    "            balanced_test_df,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('val_mean_auc', val_auc)\n",
    "        mlflow.log_metric('test_mean_auc', test_auc)\n",
    "        \n",
    "        # Log per-label AUCs\n",
    "        for label, val_auc_label, test_auc_label in zip(labels, val_label_aucs, test_label_aucs):\n",
    "            mlflow.log_metric(f'val_auc_{label}', val_auc_label)\n",
    "            mlflow.log_metric(f'test_auc_{label}', test_auc_label)\n",
    "        \n",
    "        # Log neutral performance metrics\n",
    "        for metric_name, value in val_neutral_metrics.items():\n",
    "            mlflow.log_metric(f'val_{metric_name}', value)\n",
    "        \n",
    "        for metric_name, value in test_neutral_metrics.items():\n",
    "            mlflow.log_metric(f'test_{metric_name}', value)\n",
    "        \n",
    "        results[model_config['name']] = {\n",
    "            'val_mean_auc': val_auc,\n",
    "            'test_mean_auc': test_auc,\n",
    "            'val_label_aucs': val_label_aucs,\n",
    "            'test_label_aucs': test_label_aucs,\n",
    "            'val_neutral_metrics': val_neutral_metrics,\n",
    "            'test_neutral_metrics': test_neutral_metrics\n",
    "        }\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCED EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'Val AUC':<10} {'Test AUC':<10} {'Val Neutral Acc':<15} {'Test Neutral Acc':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['val_mean_auc'], reverse=True):\n",
    "    val_neutral_acc = metrics['val_neutral_metrics'].get('neutral_accuracy', 0.0)\n",
    "    test_neutral_acc = metrics['test_neutral_metrics'].get('neutral_accuracy', 0.0)\n",
    "    \n",
    "    print(f\"{model_name:<30} {metrics['val_mean_auc']:.4f}     {metrics['test_mean_auc']:.4f}     \"\n",
    "          f\"{val_neutral_acc:.4f}          {test_neutral_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEUTRAL PERFORMANCE DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    val_neutral = metrics['val_neutral_metrics']\n",
    "    test_neutral = metrics['test_neutral_metrics']\n",
    "    \n",
    "    print(f\"  Validation - Total neutral: {val_neutral.get('neutral_total', 0)}, \"\n",
    "          f\"correct: {val_neutral.get('neutral_correct', 0)}, \"\n",
    "          f\"FP Rate: {val_neutral.get('neutral_fp_rate', 0.0):.4f}\")\n",
    "    \n",
    "    print(f\"  Test - Total neutral: {test_neutral.get('neutral_total', 0)}, \"\n",
    "          f\"correct: {test_neutral.get('neutral_correct', 0)}, \"\n",
    "          f\"FP Rate: {test_neutral.get('neutral_fp_rate', 0.0):.4f}\")\n",
    "\n",
    "print(f\"Class weights used: {dict(zip(labels, class_weights))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400873e",
   "metadata": {},
   "source": [
    "Clearly, unitary/toxic-bert has the best results.\n",
    "\n",
    "\n",
    "The model achieves very high AUCs across all toxic classes, indicating strong discrimination between toxic and non-toxic comments.\n",
    "\n",
    "On the validation set, the model perfectly classifies all neutral comments (no false positives). On the test set, it maintains a very high neutral accuracy (90.5%), with only 9.5% of neutral comments incorrectly flagged as toxic.\n",
    "\n",
    "The small drop in AUC and neutral accuracy from validation to test suggests the model generalizes well and is not overfitting to the validation data.\n",
    "\n",
    "**CONCLUSION:** Select 'unitary/toxic-bert' as baseline model and try to improve it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899ebf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10567ae7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc55b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 179631418936248154 | Name: HuggingFace_Baselines_balanced | Artifact Location: file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/179631418936248154\n",
      "ID: 167733686526390127 | Name: HuggingFace_Baselines | Artifact Location: file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/167733686526390127\n",
      "ID: 365461217584339427 | Name: MultiLabel_EDA | Artifact Location: file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/365461217584339427\n",
      "ID: 0 | Name: Default | Artifact Location: file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/0\n"
     ]
    }
   ],
   "source": [
    "experiments = client.search_experiments()\n",
    "for exp in experiments:\n",
    "    print(f\"ID: {exp.experiment_id} | Name: {exp.name} | Artifact Location: {exp.artifact_location}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
