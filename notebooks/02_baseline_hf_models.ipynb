{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f75e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0827e",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87946d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fe39",
   "metadata": {},
   "source": [
    "DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e1d0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "base_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "data_dir = base_dir / \"src\" / \"data\" / \"raw\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"train.csv\")\n",
    "test = pd.read_csv(data_dir / \"test.csv\")\n",
    "test_labels = pd.read_csv(data_dir / \"test_labels.csv\") \n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b7e08",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a477f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "def prepare_test_for_evaluation(test_df, test_labels_df):\n",
    "    valid_mask = (test_labels_df[labels] != -1).all(axis=1)\n",
    "    print(f\"Test samples: {len(test_df)} total, {valid_mask.sum()} valid for evaluation\")\n",
    "    return test_df[valid_mask].copy(), test_labels_df[valid_mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c8b73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_subset(df, labels, n_per_class=200):\n",
    "    \"\"\"\n",
    "    Crea un subset balanceado para evaluación más justa\n",
    "    \"\"\"\n",
    "    idxs = set()\n",
    "    print(f\"Creating balanced subset with {n_per_class} samples per class...\")\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # Encuentra índices donde esa clase está presente\n",
    "        class_idxs = np.where(df[label].values == 1)[0]\n",
    "        available = len(class_idxs)\n",
    "        to_sample = min(n_per_class, available)\n",
    "        \n",
    "        if available > 0:\n",
    "            chosen = np.random.choice(class_idxs, to_sample, replace=False)\n",
    "            idxs.update(chosen)\n",
    "            print(f\"  {label}: {to_sample}/{available} samples\")\n",
    "        else:\n",
    "            print(f\"  {label}: 0 samples available!\")\n",
    "    \n",
    "    # Añade algunos neutros (all zero)\n",
    "    neutral_idxs = np.where(df[labels].sum(axis=1) == 0)[0]\n",
    "    neutral_available = len(neutral_idxs)\n",
    "    neutral_to_sample = min(n_per_class, neutral_available)\n",
    "    \n",
    "    if neutral_available > 0:\n",
    "        chosen_neutral = np.random.choice(neutral_idxs, neutral_to_sample, replace=False)\n",
    "        idxs.update(chosen_neutral)\n",
    "        print(f\"  neutral: {neutral_to_sample}/{neutral_available} samples\")\n",
    "    \n",
    "    # Devuelve el subset balanceado\n",
    "    idxs = list(idxs)\n",
    "    balanced_df = df.iloc[idxs].copy()\n",
    "    print(f\"Total balanced samples: {len(balanced_df)}\")\n",
    "    return balanced_df\n",
    "\n",
    "def calculate_class_weights(train_df, labels):\n",
    "    \"\"\"\n",
    "    Calcula pesos para manejar el desbalanceo de clases\n",
    "    \"\"\"\n",
    "    class_counts = train_df[labels].sum(axis=0)\n",
    "    total = len(train_df)\n",
    "    pos_weights = (total - class_counts) / (class_counts + 1e-6)\n",
    "    \n",
    "    print(\"Class distribution and weights:\")\n",
    "    for label, count, weight in zip(labels, class_counts, pos_weights):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {label}: {count} samples ({percentage:.2f}%) -> weight: {weight:.2f}\")\n",
    "    \n",
    "    return pos_weights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "894b8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_neutral_performance(y_true, y_pred_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evalúa específicamente el rendimiento en comentarios neutros\n",
    "    \"\"\"\n",
    "    # Convertir probabilidades a predicciones binarias\n",
    "    y_pred_bin = (y_pred_probs > threshold).astype(int)\n",
    "    \n",
    "    # Identificar comentarios neutros (todas las etiquetas = 0)\n",
    "    neutral_mask = (y_true.sum(axis=1) == 0)\n",
    "    neutral_total = neutral_mask.sum()\n",
    "    \n",
    "    if neutral_total == 0:\n",
    "        return {\n",
    "            'neutral_total': 0,\n",
    "            'neutral_correct': 0,\n",
    "            'neutral_accuracy': 0.0,\n",
    "            'neutral_fp_rate': 0.0\n",
    "        }\n",
    "    \n",
    "    # ¿Cuántos neutros fueron predichos como neutros?\n",
    "    neutral_pred_mask = (y_pred_bin[neutral_mask].sum(axis=1) == 0)\n",
    "    neutral_correct = neutral_pred_mask.sum()\n",
    "    neutral_fp = neutral_total - neutral_correct\n",
    "    \n",
    "    # Calcular métricas\n",
    "    neutral_accuracy = neutral_correct / neutral_total\n",
    "    neutral_fp_rate = neutral_fp / neutral_total\n",
    "    \n",
    "    return {\n",
    "        'neutral_total': int(neutral_total),\n",
    "        'neutral_correct': int(neutral_correct),\n",
    "        'neutral_fp': int(neutral_fp),\n",
    "        'neutral_accuracy': float(neutral_accuracy),\n",
    "        'neutral_fp_rate': float(neutral_fp_rate)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a41d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4050372971.py, line 28)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbalanced_test_df = create_balanced_subset(test_eval, labels, n_per_class=200)test_df_temp = test_eval.copy()\u001b[39m\n                                                                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "test_eval, test_labels_eval = prepare_test_for_evaluation(test, test_labels)\n",
    "\n",
    "\n",
    "# Split training data\n",
    "X = train['comment_text'].values\n",
    "y = train[labels].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y[:, 0]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test evaluation size: {len(test_eval)}\")\n",
    "\n",
    "# Calcular pesos para el desbalanceo de clases\n",
    "class_weights = calculate_class_weights(train, labels)\n",
    "pos_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Crear datasets balanceados para evaluación\n",
    "print(\"Creating balanced validation set\")\n",
    "val_df_temp = pd.DataFrame({'comment_text': X_val})\n",
    "val_df_temp[labels] = y_val\n",
    "balanced_val_df = create_balanced_subset(val_df_temp, labels, n_per_class=300)\n",
    "\n",
    "print(\"Creating balanced test set\")\n",
    "test_df_temp = test_eval.copy()\n",
    "test_df_temp[labels] = test_labels_eval[labels].values\n",
    "balanced_test_df = create_balanced_subset(test_df_temp, labels, n_per_class=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e24f1",
   "metadata": {},
   "source": [
    "MODELS TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "MODELS_TO_TEST = [\n",
    "    {\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'batch_size': 32,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'roberta-base',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'microsoft/deberta-v3-base',\n",
    "        'batch_size': 8,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'unitary/toxic-bert',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb69ca",
   "metadata": {},
   "source": [
    "EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, eval_df, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on balanced validation set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        X_eval = eval_df['comment_text'].values\n",
    "        y_eval = eval_df[labels].values\n",
    "        \n",
    "        for j in tqdm(range(0, len(X_eval), batch_size), desc=f\"Evaluating {model_name}\"):\n",
    "            batch_texts = X_eval[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        # Calcular AUC por etiqueta\n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if len(np.unique(y_eval[:, i])) > 1:  # Solo si hay ambas clases\n",
    "                auc = roc_auc_score(y_eval[:, i], predictions[:, i])\n",
    "                label_aucs.append(auc)\n",
    "                print(f\"  {label}: {auc:.4f}\")\n",
    "            else:\n",
    "                label_aucs.append(0.0)\n",
    "                print(f\"  {label}: No samples or single class\")\n",
    "        \n",
    "        mean_auc = np.mean([auc for auc in label_aucs if auc > 0])\n",
    "        print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        # Evaluar rendimiento en neutros\n",
    "        neutral_metrics = evaluate_neutral_performance(y_eval, predictions)\n",
    "        print(f\"  Neutral accuracy: {neutral_metrics['neutral_accuracy']:.4f}\")\n",
    "        print(f\"  Neutral FP rate: {neutral_metrics['neutral_fp_rate']:.4f}\")\n",
    "        print(f\"  Neutral samples: {neutral_metrics['neutral_total']}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs, neutral_metrics, predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels), {}, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model_name, test_eval_df, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on balanced test set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        X_test = test_eval_df['comment_text'].values\n",
    "        y_test = test_eval_df[labels].values\n",
    "        \n",
    "        for j in tqdm(range(0, len(X_test), batch_size), desc=f\"Test evaluation\"):\n",
    "            batch_texts = X_test[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        # Calcular AUC por etiqueta\n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if len(np.unique(y_test[:, i])) > 1:\n",
    "                auc = roc_auc_score(y_test[:, i], predictions[:, i])\n",
    "                label_aucs.append(auc)\n",
    "                print(f\"  {label}: {auc:.4f}\")\n",
    "            else:\n",
    "                label_aucs.append(0.0)\n",
    "                print(f\"  {label}: No samples or single class\")\n",
    "        \n",
    "        mean_auc = np.mean([auc for auc in label_aucs if auc > 0])\n",
    "        print(f\"  Test Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        # Evaluar rendimiento en neutros\n",
    "        neutral_metrics = evaluate_neutral_performance(y_test, predictions)\n",
    "        print(f\"  Test Neutral accuracy: {neutral_metrics['neutral_accuracy']:.4f}\")\n",
    "        print(f\"  Test Neutral FP rate: {neutral_metrics['neutral_fp_rate']:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs, neutral_metrics, predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels), {}, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e7957",
   "metadata": {},
   "source": [
    "TEST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc22028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/167733686526390127', creation_time=1754472152381, experiment_id='167733686526390127', last_update_time=1754472152381, lifecycle_stage='active', name='HuggingFace_Baselines', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"HuggingFace_Baselines_balanced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Evaluating distilbert-base-uncased on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating distilbert-base-uncased: 100%|██████████| 313/313 [00:29<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.7482\n",
      "  severe_toxic: 0.2788\n",
      "  obscene: 0.5819\n",
      "  threat: 0.6947\n",
      "  insult: 0.2340\n",
      "  identity_hate: 0.4773\n",
      "  Mean AUC: 0.5025\n",
      "\n",
      "🔍 Evaluating distilbert-base-uncased on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 63/63 [00:05<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.6151\n",
      "  severe_toxic: 0.1958\n",
      "  obscene: 0.3210\n",
      "  threat: 0.8783\n",
      "  insult: 0.4823\n",
      "  identity_hate: 0.4376\n",
      "  Test Mean AUC: 0.4883\n",
      "\n",
      "🔍 Evaluating bert-base-uncased on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 556kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 412kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.38MB/s]\n",
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [02:27<00:00, 2.99MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating bert-base-uncased: 100%|██████████| 313/313 [00:54<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4053\n",
      "  severe_toxic: 0.2658\n",
      "  obscene: 0.4578\n",
      "  threat: 0.7185\n",
      "  insult: 0.5386\n",
      "  identity_hate: 0.4495\n",
      "  Mean AUC: 0.4726\n",
      "\n",
      "🔍 Evaluating bert-base-uncased on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 63/63 [00:10<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.5181\n",
      "  severe_toxic: 0.4171\n",
      "  obscene: 0.4007\n",
      "  threat: 0.6169\n",
      "  insult: 0.4938\n",
      "  identity_hate: 0.4872\n",
      "  Test Mean AUC: 0.4890\n",
      "\n",
      "🔍 Evaluating roberta-base on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 481/481 [00:00<?, ?B/s] \n",
      "Downloading vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 5.18MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 7.86MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.29MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 499M/499M [01:37<00:00, 5.12MB/s] \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating roberta-base: 100%|██████████| 313/313 [00:55<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4604\n",
      "  severe_toxic: 0.4941\n",
      "  obscene: 0.6331\n",
      "  threat: 0.8391\n",
      "  insult: 0.4797\n",
      "  identity_hate: 0.3154\n",
      "  Mean AUC: 0.5370\n",
      "\n",
      "🔍 Evaluating roberta-base on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|██████████| 63/63 [00:11<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4827\n",
      "  severe_toxic: 0.6430\n",
      "  obscene: 0.5116\n",
      "  threat: 0.3725\n",
      "  insult: 0.5086\n",
      "  identity_hate: 0.4466\n",
      "  Test Mean AUC: 0.4942\n",
      "\n",
      "🔍 Evaluating microsoft/deberta-v3-base on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 50.0kB/s]\n",
      "Downloading config.json: 100%|██████████| 579/579 [00:00<?, ?B/s] \n",
      "Downloading spm.model: 100%|██████████| 2.46M/2.46M [00:00<00:00, 9.79MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: Couldn't instantiate the backend tokenizer from one of: \n",
      "(1) a `tokenizers` library serialization file, \n",
      "(2) a slow tokenizer instance to convert or \n",
      "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
      "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
      "\n",
      "🔍 Evaluating microsoft/deberta-v3-base on test set...\n",
      "❌ Error: Couldn't instantiate the backend tokenizer from one of: \n",
      "(1) a `tokenizers` library serialization file, \n",
      "(2) a slow tokenizer instance to convert or \n",
      "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
      "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
      "\n",
      "🔍 Evaluating unitary/toxic-bert on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 174/174 [00:00<?, ?B/s] \n",
      "Downloading config.json: 100%|██████████| 811/811 [00:00<?, ?B/s] \n",
      "Downloading vocab.txt: 232kB [00:00, 9.69MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 37.3kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 438M/438M [01:25<00:00, 5.11MB/s] \n",
      "Evaluating unitary/toxic-bert: 100%|██████████| 313/313 [00:55<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.9968\n",
      "  severe_toxic: 0.9912\n",
      "  obscene: 0.9957\n",
      "  threat: 0.9980\n",
      "  insult: 0.9962\n",
      "  identity_hate: 0.9971\n",
      "  Mean AUC: 0.9958\n",
      "\n",
      "🔍 Evaluating unitary/toxic-bert on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test evaluation: 100%|██████████| 63/63 [00:11<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.9715\n",
      "  severe_toxic: 0.9741\n",
      "  obscene: 0.9833\n",
      "  threat: 1.0000\n",
      "  insult: 0.9784\n",
      "  identity_hate: 0.9922\n",
      "  Test Mean AUC: 0.9832\n",
      "\n",
      "📊 Baseline Results Summary:\n",
      "----------------------------------------------------------------------\n",
      "Model                          Val AUC    Test AUC  \n",
      "----------------------------------------------------------------------\n",
      "unitary/toxic-bert             0.9958     0.9922\n",
      "roberta-base                   0.5370     0.4466\n",
      "distilbert-base-uncased        0.5025     0.4376\n",
      "bert-base-uncased              0.4726     0.4872\n",
      "microsoft/deberta-v3-base      0.0000     0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test each model\n",
    "results = {}\n",
    "\n",
    "for model_config in MODELS_TO_TEST:\n",
    "    with mlflow.start_run(run_name=f\"balanced_{model_config['name'].split('/')[-1]}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(model_config)\n",
    "        mlflow.log_param('class_weights_used', True)\n",
    "        mlflow.log_param('balanced_evaluation', True)\n",
    "        \n",
    "        # Evaluate on balanced validation set\n",
    "        val_auc, val_label_aucs, val_neutral_metrics, _ = evaluate_model(\n",
    "            model_config['name'],\n",
    "            balanced_val_df,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # Evaluate on balanced test set\n",
    "        test_auc, test_label_aucs, test_neutral_metrics, _ = evaluate_model_on_test(\n",
    "            model_config['name'],\n",
    "            balanced_test_df,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('val_mean_auc', val_auc)\n",
    "        mlflow.log_metric('test_mean_auc', test_auc)\n",
    "        \n",
    "        # Log per-label AUCs\n",
    "        for label, val_auc_label, test_auc_label in zip(labels, val_label_aucs, test_label_aucs):\n",
    "            mlflow.log_metric(f'val_auc_{label}', val_auc_label)\n",
    "            mlflow.log_metric(f'test_auc_{label}', test_auc_label)\n",
    "        \n",
    "        # Log neutral performance metrics\n",
    "        for metric_name, value in val_neutral_metrics.items():\n",
    "            mlflow.log_metric(f'val_{metric_name}', value)\n",
    "        \n",
    "        for metric_name, value in test_neutral_metrics.items():\n",
    "            mlflow.log_metric(f'test_{metric_name}', value)\n",
    "        \n",
    "        results[model_config['name']] = {\n",
    "            'val_mean_auc': val_auc,\n",
    "            'test_mean_auc': test_auc,\n",
    "            'val_label_aucs': val_label_aucs,\n",
    "            'test_label_aucs': test_label_aucs,\n",
    "            'val_neutral_metrics': val_neutral_metrics,\n",
    "            'test_neutral_metrics': test_neutral_metrics\n",
    "        }\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCED EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'Val AUC':<10} {'Test AUC':<10} {'Val Neutral Acc':<15} {'Test Neutral Acc':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['val_mean_auc'], reverse=True):\n",
    "    val_neutral_acc = metrics['val_neutral_metrics'].get('neutral_accuracy', 0.0)\n",
    "    test_neutral_acc = metrics['test_neutral_metrics'].get('neutral_accuracy', 0.0)\n",
    "    \n",
    "    print(f\"{model_name:<30} {metrics['val_mean_auc']:.4f}     {metrics['test_mean_auc']:.4f}     \"\n",
    "          f\"{val_neutral_acc:.4f}          {test_neutral_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEUTRAL PERFORMANCE DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    val_neutral = metrics['val_neutral_metrics']\n",
    "    test_neutral = metrics['test_neutral_metrics']\n",
    "    \n",
    "    print(f\"  Validation - Total neutros: {val_neutral.get('neutral_total', 0)}, \"\n",
    "          f\"Correctos: {val_neutral.get('neutral_correct', 0)}, \"\n",
    "          f\"FP Rate: {val_neutral.get('neutral_fp_rate', 0.0):.4f}\")\n",
    "    \n",
    "    print(f\"  Test - Total neutros: {test_neutral.get('neutral_total', 0)}, \"\n",
    "          f\"Correctos: {test_neutral.get('neutral_correct', 0)}, \"\n",
    "          f\"FP Rate: {test_neutral.get('neutral_fp_rate', 0.0):.4f}\")\n",
    "\n",
    "print(f\"Class weights used: {dict(zip(labels, class_weights))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
