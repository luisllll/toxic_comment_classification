{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f75e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87946d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e1d0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "base_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "data_dir = base_dir / \"src\" / \"data\" / \"raw\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"train.csv\")\n",
    "test = pd.read_csv(data_dir / \"test.csv\")\n",
    "test_labels = pd.read_csv(data_dir / \"test_labels.csv\") \n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a477f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 153164 total, 63978 valid for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare test\n",
    "def prepare_test_for_evaluation(test_df, test_labels_df):\n",
    "    valid_mask = (test_labels_df[labels] != -1).all(axis=1)\n",
    "    print(f\"Test samples: {len(test_df)} total, {valid_mask.sum()} valid for evaluation\")\n",
    "    return test_df[valid_mask].copy(), test_labels_df[valid_mask].copy()\n",
    "\n",
    "test_eval, test_labels_eval = prepare_test_for_evaluation(test, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639a41d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 143613\n",
      "Validation size: 15958\n",
      "Test evaluation size: 63978\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "X = train['comment_text'].values\n",
    "y = train[labels].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y[:, 0]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test evaluation size: {len(test_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba0fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "MODELS_TO_TEST = [\n",
    "    {\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'batch_size': 32,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'roberta-base',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'microsoft/deberta-v3-base',\n",
    "        'batch_size': 8,\n",
    "        'max_length': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'unitary/toxic-bert',\n",
    "        'batch_size': 16,\n",
    "        'max_length': 128\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, X_val_sample, y_val_sample, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on validation set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        for j in tqdm(range(0, len(X_val_sample), batch_size), desc=f\"Evaluating {model_name}\"):\n",
    "            batch_texts = X_val_sample[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            auc = roc_auc_score(y_val_sample[:, i], predictions[:, i])\n",
    "            label_aucs.append(auc)\n",
    "            print(f\"  {label}: {auc:.4f}\")\n",
    "        \n",
    "        mean_auc = np.mean(label_aucs)\n",
    "        print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model_name, test_eval_df, test_labels_eval_df, max_length=128):\n",
    "    try:\n",
    "        print(f\"Evaluating {model_name} on test set...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        X_test = test_eval_df['comment_text'].values\n",
    "        \n",
    "        for j in tqdm(range(0, len(X_test), batch_size), desc=f\"Test evaluation\"):\n",
    "            batch_texts = X_test[j:j+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts.tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_predictions.append(probs.cpu().numpy())\n",
    "        \n",
    "        predictions = np.vstack(all_predictions)\n",
    "        \n",
    "        # Usar las etiquetas verdaderas del test set filtrado\n",
    "        y_true = test_labels_eval_df[labels].values\n",
    "        \n",
    "        label_aucs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            auc = roc_auc_score(y_true[:, i], predictions[:, i])\n",
    "            label_aucs.append(auc)\n",
    "            print(f\"  {label}: {auc:.4f}\")\n",
    "        \n",
    "        mean_auc = np.mean(label_aucs)\n",
    "        print(f\"  Test Mean AUC: {mean_auc:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return mean_auc, label_aucs, predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return 0.0, [0.0] * len(labels), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc22028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/wd/wd_demos/toxic_comment_classification/notebooks/mlruns/167733686526390127', creation_time=1754472152381, experiment_id='167733686526390127', last_update_time=1754472152381, lifecycle_stage='active', name='HuggingFace_Baselines', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"HuggingFace_Baselines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating distilbert-base-uncased on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:29<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.7482\n",
      "  severe_toxic: 0.2788\n",
      "  obscene: 0.5819\n",
      "  threat: 0.6947\n",
      "  insult: 0.2340\n",
      "  identity_hate: 0.4773\n",
      "  Mean AUC: 0.5025\n",
      "\n",
      "üîç Evaluating distilbert-base-uncased on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:05<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.6151\n",
      "  severe_toxic: 0.1958\n",
      "  obscene: 0.3210\n",
      "  threat: 0.8783\n",
      "  insult: 0.4823\n",
      "  identity_hate: 0.4376\n",
      "  Test Mean AUC: 0.4883\n",
      "\n",
      "üîç Evaluating bert-base-uncased on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 556kB/s]\n",
      "Downloading vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 412kB/s]\n",
      "Downloading tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 1.38MB/s]\n",
      "c:\\Users\\analyst4\\AppData\\Local\\anaconda3\\envs\\toxic_py311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440M/440M [02:27<00:00, 2.99MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating bert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:54<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.4053\n",
      "  severe_toxic: 0.2658\n",
      "  obscene: 0.4578\n",
      "  threat: 0.7185\n",
      "  insult: 0.5386\n",
      "  identity_hate: 0.4495\n",
      "  Mean AUC: 0.4726\n",
      "\n",
      "üîç Evaluating bert-base-uncased on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:10<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toxic: 0.5181\n",
      "  severe_toxic: 0.4171\n",
      "  obscene: 0.4007\n",
      "  threat: 0.6169\n",
      "  insult: 0.4938\n",
      "  identity_hate: 0.4872\n",
      "  Test Mean AUC: 0.4890\n",
      "\n",
      "üîç Evaluating roberta-base on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0/25.0 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481/481 [00:00<?, ?B/s] \n",
      "Downloading vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 5.18MB/s]\n",
      "Downloading merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 7.86MB/s]\n",
      "Downloading tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 3.29MB/s]\n",
      "Downloading model.safetensors:  23%|‚ñà‚ñà‚ñé       | 115M/499M [00:17<01:11, 5.34MB/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "# Test each model\n",
    "results = {}\n",
    "\n",
    "# Use a sample for quick baseline evaluation\n",
    "sample_size = 5000\n",
    "idx = np.random.choice(len(X_val), sample_size, replace=False)\n",
    "X_val_sample = X_val[idx]\n",
    "y_val_sample = y_val[idx]\n",
    "\n",
    "for model_config in MODELS_TO_TEST:\n",
    "    with mlflow.start_run(run_name=f\"baseline_{model_config['name'].split('/')[-1]}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(model_config)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        mean_auc, label_aucs = evaluate_model(\n",
    "            model_config['name'],\n",
    "            X_val_sample,\n",
    "            y_val_sample,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # evaluate on test\n",
    "        test_sample_size = min(1000, len(test_eval))\n",
    "        test_sample = test_eval.sample(test_sample_size, random_state=42)\n",
    "        test_labels_sample = test_labels_eval.loc[test_sample.index]\n",
    "        \n",
    "        test_auc, test_label_aucs, _ = evaluate_model_on_test(\n",
    "            model_config['name'],\n",
    "            test_sample,\n",
    "            test_labels_sample,\n",
    "            model_config['max_length']\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('val_mean_auc', mean_auc)\n",
    "        mlflow.log_metric('test_mean_auc', test_auc)\n",
    "        \n",
    "        for label, auc, test_auc in zip(labels, label_aucs, test_label_aucs):\n",
    "            mlflow.log_metric(f'val_auc_{label}', auc)\n",
    "            mlflow.log_metric(f'test_auc_{label}', test_auc)\n",
    "        \n",
    "        results[model_config['name']] = {\n",
    "            'val_mean_auc': mean_auc,\n",
    "            'test_mean_auc': test_auc,\n",
    "            'val_label_aucs': label_aucs,\n",
    "            'test_label_aucs': test_label_aucs\n",
    "        }\n",
    "\n",
    "# Display results summary\n",
    "print(\"Baseline Results Summary:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<30} {'Val AUC':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['val_mean_auc'], reverse=True):\n",
    "    print(f\"{model_name:<30} {metrics['val_mean_auc']:.4f}     {metrics['test_mean_auc']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
