{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "notebook_header",
      "metadata": {},
      "source": [
        "# Toxic-BERT Fine-Tuning with Balanced Classes\n",
        "====\n",
        "\n",
        "This notebook implements fine-tuning of `unitary/toxic-bert` using balanced classes and compares:\n",
        "1. **Baseline**: Original pre-trained Toxic-BERT\n",
        "2. **Fine-tuned**: Toxic-BERT fine-tuned on balanced dataset\n",
        "3. **Fine-tuned + Tagging**: Fine-tuned model with SUBTLE_TOXICITY tagging method\n",
        "\n",
        "## Objectives:\n",
        "- Address class imbalance in toxicity detection\n",
        "- Evaluate impact of fine-tuning on model performance\n",
        "- Test if subtle toxicity tagging helps fine-tuned models\n",
        "- Provide comprehensive performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0807 18:26:50.547000 22088 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 2050\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, classification_report, confusion_matrix,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Experiment tracking\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config_section",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully!\n",
            "Training with 500 samples per class\n",
            "Total balanced samples: ~3500\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    # Model settings\n",
        "    'model_name': 'unitary/toxic-bert',\n",
        "    'max_length': 128,  # Reduced from 512 - significant speedup\n",
        "    'num_labels': 6,\n",
        "    \n",
        "    # Training settings\n",
        "    'batch_size': 64,  # Increased from 16 - better GPU utilization\n",
        "    'learning_rate': 5e-5,  # Slightly higher for faster convergence\n",
        "    'num_epochs': 1,  # Reduced from 3\n",
        "    'warmup_steps': 200,  # Reduced from 500\n",
        "    'weight_decay': 0.01,\n",
        "    'gradient_accumulation_steps': 1,  # Reduced from 2 for faster updates\n",
        "    \n",
        "    # Data settings\n",
        "    'samples_per_class': 250,  # Reduced from 2000 - smaller dataset\n",
        "    'test_size': 0.2,\n",
        "    'val_size': 0.1,\n",
        "    'random_state': 42,\n",
        "    \n",
        "    # Evaluation settings\n",
        "    'eval_steps': 200,  # More frequent evaluation for early stopping\n",
        "    'save_steps': 400,  \n",
        "    'logging_steps': 50,  # More frequent logging\n",
        "    'early_stopping_patience': 2,  # Reduced from 3 for faster stopping\n",
        "    \n",
        "    # Output settings\n",
        "    'output_dir': './models/toxic_bert_finetuned',\n",
        "    'save_total_limit': 1,  # Keep fewer checkpoints\n",
        "}\n",
        "\n",
        "\n",
        "# Labels\n",
        "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Training with {CONFIG['samples_per_class']} samples per class\")\n",
        "print(f\"Total balanced samples: ~{CONFIG['samples_per_class'] * (len(LABELS) + 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data_loading",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "load_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded from: c:\\wd\\wd_demos\\toxic_comment_classification\\src\\data\\raw\n",
            "Train samples: 159571\n",
            "Test samples: 153164\n",
            "Labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "\n",
            "Original class distribution:\n",
            "  toxic: 15,294 (9.58%)\n",
            "  severe_toxic: 1,595 (1.00%)\n",
            "  obscene: 8,449 (5.29%)\n",
            "  threat: 478 (0.30%)\n",
            "  insult: 7,877 (4.94%)\n",
            "  identity_hate: 1,405 (0.88%)\n",
            "  neutral: 143,346 (89.83%)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "def load_toxic_data():\n",
        "    \"\"\"Load and prepare toxic comment dataset.\"\"\"\n",
        "    base_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
        "    data_dir = base_dir / \"src\" / \"data\" / \"raw\"\n",
        "    \n",
        "    # Try different possible paths\n",
        "    possible_paths = [\n",
        "        data_dir,\n",
        "        Path(\"../data/raw\"),\n",
        "        Path(\"./data/raw\"),\n",
        "        Path(\"../src/data/raw\")\n",
        "    ]\n",
        "    \n",
        "    train_df = None\n",
        "    for path in possible_paths:\n",
        "        try:\n",
        "            if (path / \"train.csv\").exists():\n",
        "                train_df = pd.read_csv(path / \"train.csv\")\n",
        "                test_df = pd.read_csv(path / \"test.csv\")\n",
        "                test_labels_df = pd.read_csv(path / \"test_labels.csv\")\n",
        "                print(f\"Data loaded from: {path}\")\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if train_df is None:\n",
        "        raise FileNotFoundError(\"Could not find train.csv. Please ensure data is in the correct location.\")\n",
        "    \n",
        "    return train_df, test_df, test_labels_df\n",
        "\n",
        "# Load the data\n",
        "train_df, test_df, test_labels_df = load_toxic_data()\n",
        "\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(f\"Labels: {LABELS}\")\n",
        "\n",
        "# Display class distribution\n",
        "print(\"\\nOriginal class distribution:\")\n",
        "for label in LABELS:\n",
        "    count = train_df[label].sum()\n",
        "    percentage = (count / len(train_df)) * 100\n",
        "    print(f\"  {label}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "neutral_count = (train_df[LABELS].sum(axis=1) == 0).sum()\n",
        "print(f\"  neutral: {neutral_count:,} ({(neutral_count/len(train_df))*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "create_balanced_dataset",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating balanced dataset with 500 samples per class...\n",
            "  toxic: 500/15,294 samples\n",
            "  severe_toxic: 500/1,595 samples\n",
            "  obscene: 500/8,449 samples\n",
            "  threat: 478/478 samples\n",
            "  insult: 500/7,877 samples\n",
            "  identity_hate: 500/1,405 samples\n",
            "  neutral: 500/143,346 samples\n",
            "\n",
            "Balanced dataset created with 3,173 total samples\n",
            "\n",
            "Balanced class distribution:\n",
            "  toxic: 2,535 (79.89%)\n",
            "  severe_toxic: 793 (24.99%)\n",
            "  obscene: 1,991 (62.75%)\n",
            "  threat: 478 (15.06%)\n",
            "  insult: 1,955 (61.61%)\n",
            "  identity_hate: 705 (22.22%)\n",
            "  neutral: 500 (15.76%)\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df, labels, samples_per_class=2000, random_state=42):\n",
        "    \"\"\"Create a balanced dataset with equal samples per class.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    balanced_indices = set()\n",
        "    \n",
        "    print(f\"Creating balanced dataset with {samples_per_class} samples per class...\")\n",
        "    \n",
        "    # Sample from each toxic class\n",
        "    for label in labels:\n",
        "        positive_indices = df[df[label] == 1].index.tolist()\n",
        "        available = len(positive_indices)\n",
        "        to_sample = min(samples_per_class, available)\n",
        "        \n",
        "        if available > 0:\n",
        "            chosen = np.random.choice(positive_indices, to_sample, replace=False)\n",
        "            balanced_indices.update(chosen)\n",
        "            print(f\"  {label}: {to_sample:,}/{available:,} samples\")\n",
        "        else:\n",
        "            print(f\"  {label}: 0 samples available!\")\n",
        "    \n",
        "    # Sample neutral examples\n",
        "    neutral_indices = df[df[labels].sum(axis=1) == 0].index.tolist()\n",
        "    neutral_available = len(neutral_indices)\n",
        "    neutral_to_sample = min(samples_per_class, neutral_available)\n",
        "    \n",
        "    if neutral_available > 0:\n",
        "        chosen_neutral = np.random.choice(neutral_indices, neutral_to_sample, replace=False)\n",
        "        balanced_indices.update(chosen_neutral)\n",
        "        print(f\"  neutral: {neutral_to_sample:,}/{neutral_available:,} samples\")\n",
        "    \n",
        "    # Create balanced dataframe\n",
        "    balanced_df = df.loc[list(balanced_indices)].copy().reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nBalanced dataset created with {len(balanced_df):,} total samples\")\n",
        "    \n",
        "    # Show new distribution\n",
        "    print(\"\\nBalanced class distribution:\")\n",
        "    for label in labels:\n",
        "        count = balanced_df[label].sum()\n",
        "        percentage = (count / len(balanced_df)) * 100\n",
        "        print(f\"  {label}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    neutral_count = (balanced_df[labels].sum(axis=1) == 0).sum()\n",
        "    print(f\"  neutral: {neutral_count:,} ({(neutral_count/len(balanced_df))*100:.2f}%)\")\n",
        "    \n",
        "    return balanced_df\n",
        "\n",
        "# Create balanced dataset\n",
        "balanced_train_df = create_balanced_dataset(\n",
        "    train_df, \n",
        "    LABELS, \n",
        "    samples_per_class=CONFIG['samples_per_class'],\n",
        "    random_state=CONFIG['random_state']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset_class",
      "metadata": {},
      "source": [
        "## Dataset and DataLoader Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dataset_setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data splits created:\n",
            "  Train: 2,220 samples\n",
            "  Validation: 318 samples\n",
            "  Test: 635 samples\n"
          ]
        }
      ],
      "source": [
        "class ToxicCommentDataset(Dataset):\n",
        "    \"\"\"Dataset class for toxic comments.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        labels = self.labels[idx]\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "def create_data_splits(df, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
        "    \"\"\"Create train/val/test splits.\"\"\"\n",
        "    X = df['comment_text'].values\n",
        "    y = df[labels].values\n",
        "    \n",
        "    # Create stratification key (use toxic label as primary)\n",
        "    stratify_key = y[:, 0]  # toxic label\n",
        "    \n",
        "    # First split: train+val vs test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, \n",
        "        stratify=stratify_key\n",
        "    )\n",
        "    \n",
        "    # Second split: train vs val\n",
        "    stratify_temp = y_temp[:, 0]\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, \n",
        "        random_state=random_state, stratify=stratify_temp\n",
        "    )\n",
        "    \n",
        "    print(f\"Data splits created:\")\n",
        "    print(f\"  Train: {len(X_train):,} samples\")\n",
        "    print(f\"  Validation: {len(X_val):,} samples\")\n",
        "    print(f\"  Test: {len(X_test):,} samples\")\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Create data splits\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = create_data_splits(\n",
        "    balanced_train_df, \n",
        "    LABELS,\n",
        "    test_size=CONFIG['test_size'],\n",
        "    val_size=CONFIG['val_size'],\n",
        "    random_state=CONFIG['random_state']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_setup",
      "metadata": {},
      "source": [
        "## Model and Tokenizer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "model_setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded: unitary/toxic-bert\n",
            "Vocab size: 30522\n",
            "Max length: 256\n",
            "\n",
            "Datasets created:\n",
            "  Train dataset: 2220 samples\n",
            "  Val dataset: 318 samples\n",
            "  Test dataset: 635 samples\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Add padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {CONFIG['model_name']}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Max length: {CONFIG['max_length']}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ToxicCommentDataset(\n",
        "    X_train, y_train, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "val_dataset = ToxicCommentDataset(\n",
        "    X_val, y_val, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "test_dataset = ToxicCommentDataset(\n",
        "    X_test, y_test, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "\n",
        "print(f\"\\nDatasets created:\")\n",
        "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
        "print(f\"  Test dataset: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baseline_evaluation",
      "metadata": {},
      "source": [
        "## Baseline Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "baseline_eval",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading baseline Toxic-BERT model...\n",
            "Model loaded on cuda\n",
            "Model parameters: 109,486,854\n",
            "\n",
            "Evaluating baseline model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 20/20 [00:31<00:00,  1.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BASELINE MODEL RESULTS ===\n",
            "Mean AUC: 0.9601\n",
            "Neutral Accuracy: 0.9902\n",
            "Neutral FP Rate: 0.0098\n",
            "\n",
            "Per-label AUC:\n",
            "  toxic: 0.9857\n",
            "  severe_toxic: 0.8851\n",
            "  obscene: 0.9894\n",
            "  threat: 0.9773\n",
            "  insult: 0.9521\n",
            "  identity_hate: 0.9709\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, dataset, batch_size=32, device='cuda'):\n",
        "    \"\"\"Evaluate model on dataset.\"\"\"\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.sigmoid(outputs.logits)\n",
        "            \n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "    \n",
        "    predictions = np.vstack(all_predictions)\n",
        "    labels = np.vstack(all_labels)\n",
        "    \n",
        "    return predictions, labels\n",
        "\n",
        "def compute_metrics(predictions, labels, label_names):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Per-label AUC\n",
        "    aucs = []\n",
        "    for i, label in enumerate(label_names):\n",
        "        if len(np.unique(labels[:, i])) > 1:  # Check if both classes present\n",
        "            auc = roc_auc_score(labels[:, i], predictions[:, i])\n",
        "            results[f'auc_{label}'] = auc\n",
        "            aucs.append(auc)\n",
        "        else:\n",
        "            results[f'auc_{label}'] = 0.0\n",
        "            aucs.append(0.0)\n",
        "    \n",
        "    results['mean_auc'] = np.mean(aucs)\n",
        "    \n",
        "    # Overall metrics\n",
        "    binary_preds = (predictions > 0.5).astype(int)\n",
        "    \n",
        "    # Accuracy for samples with no positive labels (neutral)\n",
        "    neutral_mask = labels.sum(axis=1) == 0\n",
        "    if neutral_mask.sum() > 0:\n",
        "        neutral_preds = binary_preds[neutral_mask]\n",
        "        neutral_accuracy = (neutral_preds.sum(axis=1) == 0).mean()\n",
        "        results['neutral_accuracy'] = neutral_accuracy\n",
        "        results['neutral_fp_rate'] = 1 - neutral_accuracy\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Load baseline model\n",
        "print(\"Loading baseline Toxic-BERT model...\")\n",
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=CONFIG['num_labels'],\n",
        "    problem_type=\"multi_label_classification\"\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in baseline_model.parameters()):,}\")\n",
        "\n",
        "# Evaluate baseline on test set\n",
        "print(\"\\nEvaluating baseline model...\")\n",
        "baseline_predictions, baseline_labels = evaluate_model(\n",
        "    baseline_model, test_dataset, batch_size=CONFIG['batch_size'], device=device\n",
        ")\n",
        "\n",
        "baseline_metrics = compute_metrics(baseline_predictions, baseline_labels, LABELS)\n",
        "\n",
        "print(\"\\n=== BASELINE MODEL RESULTS ===\")\n",
        "print(f\"Mean AUC: {baseline_metrics['mean_auc']:.4f}\")\n",
        "print(f\"Neutral Accuracy: {baseline_metrics.get('neutral_accuracy', 0):.4f}\")\n",
        "print(f\"Neutral FP Rate: {baseline_metrics.get('neutral_fp_rate', 0):.4f}\")\n",
        "print(\"\\nPer-label AUC:\")\n",
        "for label in LABELS:\n",
        "    print(f\"  {label}: {baseline_metrics[f'auc_{label}']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fine_tuning",
      "metadata": {},
      "source": [
        "## Fine-Tuning Setup and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "training_setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training setup completed!\n"
          ]
        }
      ],
      "source": [
        "class MultiLabelTrainer(Trainer):\n",
        "    \"\"\"Custom trainer for multi-label classification.\"\"\"\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "        \n",
        "        # Multi-label classification loss\n",
        "        loss_fct = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def setup_training_args(output_dir):\n",
        "    \"\"\"Setup training arguments.\"\"\"\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=CONFIG['num_epochs'],\n",
        "        per_device_train_batch_size=CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "        learning_rate=CONFIG['learning_rate'],\n",
        "        weight_decay=CONFIG['weight_decay'],\n",
        "        warmup_steps=CONFIG['warmup_steps'],\n",
        "        logging_steps=CONFIG['logging_steps'],\n",
        "        eval_steps=CONFIG['eval_steps'],\n",
        "        save_steps=CONFIG['save_steps'],\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        save_total_limit=CONFIG['save_total_limit'],\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        report_to=None,  # Disable wandb\n",
        "        dataloader_pin_memory=False,\n",
        "    )\n",
        "\n",
        "# Setup MLflow experiment\n",
        "mlflow.set_experiment('Toxic_BERT_Fine_Tuning_Comparison')\n",
        "\n",
        "print(\"Training setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fine_tune_model",
      "metadata": {},
      "source": [
        "## Fine-Tune Model (Without Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fine_tune_training",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/07 18:27:59 INFO mlflow.tracking.fluent: Experiment with name 'finetuning_experiment_1754584079' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning without tagging...\n",
            "Started new MLflow run: 3066627400f64987a1d81bdaaf330273\n",
            "Skipping parameter logging to avoid MLflow conflicts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/140 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting Epoch 1/2 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▋        | 23/140 [09:02<43:19, 22.22s/it]"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Fine-tune model without tagging - FIXED\n",
        "print(\"Starting fine-tuning without tagging...\")\n",
        "\n",
        "# Disable wandb to avoid API key prompts\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Force end any existing runs and clear MLflow state - ENHANCED\n",
        "try:\n",
        "    # End all possible active runs\n",
        "    while mlflow.active_run():\n",
        "        mlflow.end_run()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# COMPLETE RESET - Kill any lingering MLflow state\n",
        "try:\n",
        "    mlflow.end_run(status='KILLED')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Set a new experiment to ensure clean state\n",
        "experiment_name = f\"finetuning_experiment_{int(time.time())}\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Create a completely unique run name with more precision\n",
        "import time\n",
        "import random\n",
        "timestamp = int(time.time() * 1000)  # More precision with milliseconds\n",
        "run_name = f\"Fine_Tuned_Model_{timestamp}_{random.randint(10000, 99999)}\"\n",
        "\n",
        "# Force create a completely new run\n",
        "with mlflow.start_run(run_name=run_name) as run:\n",
        "    print(f\"Started new MLflow run: {run.info.run_id}\")\n",
        "    \n",
        "    # Skip parameter logging to avoid conflicts\n",
        "    print(\"Skipping parameter logging to avoid MLflow conflicts...\")\n",
        "    \n",
        "    # Load fresh model for fine-tuning\n",
        "    finetuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        CONFIG['model_name'],\n",
        "        num_labels=CONFIG['num_labels'],\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    ).to(device)\n",
        "    \n",
        "    # Setup training arguments\n",
        "    training_args = setup_training_args(CONFIG['output_dir'])\n",
        "    \n",
        "    # Disable wandb reporting to avoid API key prompts\n",
        "    training_args.report_to = []\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = MultiLabelTrainer(\n",
        "        model=finetuned_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])]\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    print(\"Training started...\")\n",
        "    \n",
        "    # Add progress callback for training\n",
        "    from transformers import TrainerCallback\n",
        "    \n",
        "    \n",
        "    class ProgressCallback(TrainerCallback):\n",
        "        def __init__(self):\n",
        "            self.start_time = time.time()\n",
        "        \n",
        "        def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "            print(f\"\\n=== Starting Epoch {state.epoch + 1}/{args.num_train_epochs} ===\")\n",
        "        \n",
        "        def on_step_end(self, args, state, control, **kwargs):\n",
        "            if state.global_step % 50 == 0:  # Print every 50 steps\n",
        "                elapsed = time.time() - self.start_time\n",
        "                print(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('train_loss', 'N/A'):.4f} | \"\n",
        "                      f\"Time: {elapsed/60:.1f} min\")\n",
        "        \n",
        "        def on_evaluate(self, args, state, control, **kwargs):\n",
        "            if state.log_history:\n",
        "                last_log = state.log_history[-1]\n",
        "                eval_loss = last_log.get('eval_loss', 'N/A')\n",
        "                print(f\"Evaluation at step {state.global_step}: Eval Loss = {eval_loss}\")\n",
        "    \n",
        "    # Add progress callback to trainer\n",
        "    trainer.add_callback(ProgressCallback())\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(CONFIG['output_dir'])\n",
        "    \n",
        "    print(\"Fine-tuning completed!\")\n",
        "    print(f\"Model saved to: {CONFIG['output_dir']}\")\n",
        "    \n",
        "    # Log model safely - FIXED\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(finetuned_model, \"model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model to MLflow: {e}\")\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"\\nEvaluating fine-tuned model...\")\n",
        "finetuned_predictions, finetuned_labels = evaluate_model(\n",
        "    finetuned_model, test_dataset, batch_size=CONFIG['batch_size'], device=device\n",
        ")\n",
        "\n",
        "finetuned_metrics = compute_metrics(finetuned_predictions, finetuned_labels, LABELS)\n",
        "\n",
        "print(\"\\n=== FINE-TUNED MODEL RESULTS ===\")\n",
        "print(f\"Mean AUC: {finetuned_metrics['mean_auc']:.4f}\")\n",
        "print(f\"Neutral Accuracy: {finetuned_metrics.get('neutral_accuracy', 0):.4f}\")\n",
        "print(f\"Neutral FP Rate: {finetuned_metrics.get('neutral_fp_rate', 0):.4f}\")\n",
        "print(\"\\nPer-label AUC:\")\n",
        "for label in LABELS:\n",
        "    print(f\"  {label}: {finetuned_metrics[f'auc_{label}']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subtle_tagging",
      "metadata": {},
      "source": [
        "## Subtle Toxicity Tagging Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "subtle_tagger",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class SubtleToxicityTagger:\n",
        "    \"\"\"Implementation of SUBTLE_TOXICITY tagging method.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Passive-aggressive patterns\n",
        "        self.passive_aggressive_patterns = [\n",
        "            r\"\\bjust saying\\b\",\n",
        "            r\"\\bno offense\\b\",\n",
        "            r\"\\bwith all due respect\\b\",\n",
        "            r\"\\bi'm just\\b\",\n",
        "            r\"\\bbless your heart\\b\",\n",
        "            r\"\\bgood for you\\b\",\n",
        "            r\"\\bwhatever\\b\",\n",
        "            r\"\\bif you say so\\b\",\n",
        "            r\"\\bsure thing\\b\",\n",
        "            r\"\\bokay then\\b\"\n",
        "        ]\n",
        "        \n",
        "        # Sarcastic indicators\n",
        "        self.sarcastic_patterns = [\n",
        "            r\"\\boh really\\b\",\n",
        "            r\"\\bhow original\\b\",\n",
        "            r\"\\bwow\\b.*\\bgenius\\b\",\n",
        "            r\"\\bbrilliant\\b\",\n",
        "            r\"\\bfascinating\\b\",\n",
        "            r\"\\bimpressive\\b\",\n",
        "            r\"\\bcongratulations\\b\",\n",
        "            r\"\\bgood job\\b\",\n",
        "            r\"\\bwell done\\b\",\n",
        "            r\"\\bobviously\\b\"\n",
        "        ]\n",
        "        \n",
        "        # Dismissive language\n",
        "        self.dismissive_patterns = [\n",
        "            r\"\\byou don't understand\\b\",\n",
        "            r\"\\byou wouldn't get it\\b\",\n",
        "            r\"\\bnever mind\\b\",\n",
        "            r\"\\bforget it\\b\",\n",
        "            r\"\\bdon't bother\\b\",\n",
        "            r\"\\bwaste of time\\b\",\n",
        "            r\"\\bpointless\\b\",\n",
        "            r\"\\buseless\\b\",\n",
        "            r\"\\bwhatever\\b\",\n",
        "            r\"\\bwho cares\\b\"\n",
        "        ]\n",
        "        \n",
        "        # Condescending phrases\n",
        "        self.condescending_patterns = [\n",
        "            r\"\\blet me explain\\b\",\n",
        "            r\"\\byou need to understand\\b\",\n",
        "            r\"\\bactually\\b\",\n",
        "            r\"\\bwell actually\\b\",\n",
        "            r\"\\bfor your information\\b\",\n",
        "            r\"\\bfyi\\b\",\n",
        "            r\"\\bas i said\\b\",\n",
        "            r\"\\blike i said\\b\",\n",
        "            r\"\\bclearly\\b\",\n",
        "            r\"\\bobviously\\b\"\n",
        "        ]\n",
        "        \n",
        "        # Compile patterns for efficiency\n",
        "        self.compiled_patterns = {\n",
        "            'passive_aggressive': [re.compile(p, re.IGNORECASE) for p in self.passive_aggressive_patterns],\n",
        "            'sarcastic': [re.compile(p, re.IGNORECASE) for p in self.sarcastic_patterns],\n",
        "            'dismissive': [re.compile(p, re.IGNORECASE) for p in self.dismissive_patterns],\n",
        "            'condescending': [re.compile(p, re.IGNORECASE) for p in self.condescending_patterns]\n",
        "        }\n",
        "    \n",
        "    def detect_patterns(self, text: str) -> Dict[str, int]:\n",
        "        \"\"\"Detect subtle toxicity patterns in text.\"\"\"\n",
        "        pattern_counts = {\n",
        "            'passive_aggressive': 0,\n",
        "            'sarcastic': 0,\n",
        "            'dismissive': 0,\n",
        "            'condescending': 0\n",
        "        }\n",
        "        \n",
        "        for pattern_type, patterns in self.compiled_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                matches = pattern.findall(text)\n",
        "                pattern_counts[pattern_type] += len(matches)\n",
        "        \n",
        "        return pattern_counts\n",
        "    \n",
        "    def calculate_intensity(self, pattern_counts: Dict[str, int], text_length: int) -> str:\n",
        "        \"\"\"Calculate subtle toxicity intensity.\"\"\"\n",
        "        total_patterns = sum(pattern_counts.values())\n",
        "        \n",
        "        if total_patterns == 0:\n",
        "            return \"NONE\"\n",
        "        \n",
        "        # Normalize by text length (patterns per 100 characters)\n",
        "        if text_length > 0:\n",
        "            pattern_density = (total_patterns / text_length) * 100\n",
        "        else:\n",
        "            pattern_density = 0\n",
        "        \n",
        "        # Determine intensity based on pattern count and density\n",
        "        if total_patterns >= 3 or pattern_density > 5:\n",
        "            return \"HIGH\"\n",
        "        elif total_patterns >= 1 or pattern_density > 2:\n",
        "            return \"MEDIUM\"\n",
        "        else:\n",
        "            return \"NONE\"\n",
        "    \n",
        "    def apply_tagging(self, text: str) -> str:\n",
        "        \"\"\"Apply subtle toxicity tagging to text.\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return str(text) if text else \"\"\n",
        "        \n",
        "        # Detect patterns\n",
        "        pattern_counts = self.detect_patterns(text)\n",
        "        \n",
        "        # Calculate intensity\n",
        "        intensity = self.calculate_intensity(pattern_counts, len(text))\n",
        "        \n",
        "        # Apply tagging if patterns detected\n",
        "        if intensity != \"NONE\":\n",
        "            # Add intensity tag\n",
        "            tagged_text = f\"[SUBTLE_TOXICITY:{intensity}] {text}\"\n",
        "            \n",
        "            # Add specific pattern tags\n",
        "            pattern_tags = []\n",
        "            for pattern_type, count in pattern_counts.items():\n",
        "                if count > 0:\n",
        "                    pattern_tags.append(f\"[{pattern_type.upper()}]\")\n",
        "            \n",
        "            if pattern_tags:\n",
        "                tagged_text = \" \".join(pattern_tags) + \" \" + tagged_text\n",
        "            \n",
        "            return tagged_text\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def get_statistics(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Get tagging statistics for a list of texts.\"\"\"\n",
        "        stats = {\n",
        "            'total_texts': len(texts),\n",
        "            'tagged_texts': 0,\n",
        "            'intensity_distribution': {'NONE': 0, 'MEDIUM': 0, 'HIGH': 0},\n",
        "            'pattern_distribution': {\n",
        "                'passive_aggressive': 0,\n",
        "                'sarcastic': 0,\n",
        "                'dismissive': 0,\n",
        "                'condescending': 0\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        for text in texts:\n",
        "            if not text or not isinstance(text, str):\n",
        "                continue\n",
        "                \n",
        "            pattern_counts = self.detect_patterns(text)\n",
        "            intensity = self.calculate_intensity(pattern_counts, len(text))\n",
        "            \n",
        "            stats['intensity_distribution'][intensity] += 1\n",
        "            \n",
        "            if intensity != \"NONE\":\n",
        "                stats['tagged_texts'] += 1\n",
        "                \n",
        "                for pattern_type, count in pattern_counts.items():\n",
        "                    if count > 0:\n",
        "                        stats['pattern_distribution'][pattern_type] += 1\n",
        "        \n",
        "        return stats\n",
        "\n",
        "# Initialize the tagger\n",
        "subtle_tagger = SubtleToxicityTagger()\n",
        "\n",
        "# Test the tagger with some examples\n",
        "test_examples = [\n",
        "    \"This is a normal comment.\",\n",
        "    \"Well actually, you don't understand how this works.\",\n",
        "    \"Whatever, good for you I guess.\",\n",
        "    \"Oh really? How original. Brilliant work there.\",\n",
        "    \"Let me explain this to you since you clearly don't get it.\"\n",
        "]\n",
        "\n",
        "print(\"=== SUBTLE TOXICITY TAGGER EXAMPLES ===\")\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    tagged = subtle_tagger.apply_tagging(example)\n",
        "    print(f\"{i}. Original: {example}\")\n",
        "    print(f\"   Tagged:   {tagged}\")\n",
        "    print()\n",
        "\n",
        "# Get statistics for training data\n",
        "train_stats = subtle_tagger.get_statistics(X_train.tolist())\n",
        "print(\"=== TRAINING DATA TAGGING STATISTICS ===\")\n",
        "print(f\"Total texts: {train_stats['total_texts']:,}\")\n",
        "print(f\"Tagged texts: {train_stats['tagged_texts']:,} ({train_stats['tagged_texts']/train_stats['total_texts']*100:.2f}%)\")\n",
        "print(\"\\nIntensity distribution:\")\n",
        "for intensity, count in train_stats['intensity_distribution'].items():\n",
        "    print(f\"  {intensity}: {count:,} ({count/train_stats['total_texts']*100:.2f}%)\")\n",
        "print(\"\\nPattern distribution:\")\n",
        "for pattern, count in train_stats['pattern_distribution'].items():\n",
        "    print(f\"  {pattern}: {count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tagged_dataset",
      "metadata": {},
      "source": [
        "## Create Tagged Dataset and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_tagged_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply tagging to training data\n",
        "print(\"Applying subtle toxicity tagging to training data...\")\n",
        "X_train_tagged = np.array([subtle_tagger.apply_tagging(text) for text in X_train])\n",
        "X_val_tagged = np.array([subtle_tagger.apply_tagging(text) for text in X_val])\n",
        "X_test_tagged = np.array([subtle_tagger.apply_tagging(text) for text in X_test])\n",
        "\n",
        "print(f\"Tagged {len(X_train_tagged)} training samples\")\n",
        "print(f\"Tagged {len(X_val_tagged)} validation samples\")\n",
        "print(f\"Tagged {len(X_test_tagged)} test samples\")\n",
        "\n",
        "# Create tagged datasets\n",
        "train_dataset_tagged = ToxicCommentDataset(\n",
        "    X_train_tagged, y_train, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "val_dataset_tagged = ToxicCommentDataset(\n",
        "    X_val_tagged, y_val, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "test_dataset_tagged = ToxicCommentDataset(\n",
        "    X_test_tagged, y_test, tokenizer, CONFIG['max_length']\n",
        ")\n",
        "\n",
        "print(\"Tagged datasets created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_tagged_model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune model with tagging - FIXED\n",
        "print(\"Starting fine-tuning with subtle toxicity tagging...\")\n",
        "\n",
        "# Disable wandb to avoid API key prompts\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Force end any existing runs and clear MLflow state\n",
        "try:\n",
        "    while mlflow.active_run():\n",
        "        mlflow.end_run()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    mlflow.end_run(status='KILLED')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Set a new experiment to ensure clean state\n",
        "experiment_name = f\"tagged_experiment_{int(time.time())}\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Create a completely unique run name\n",
        "import time\n",
        "import random\n",
        "timestamp = int(time.time() * 1000)\n",
        "run_name = f\"Fine_Tuned_Tagged_Model_{timestamp}_{random.randint(10000, 99999)}\"\n",
        "\n",
        "with mlflow.start_run(run_name=run_name) as run:\n",
        "    print(f\"Started new MLflow run: {run.info.run_id}\")\n",
        "    \n",
        "    # Skip parameter logging to avoid conflicts\n",
        "    print(\"Skipping parameter logging to avoid MLflow conflicts...\")\n",
        "    \n",
        "    # Load fresh model for fine-tuning with tagging\n",
        "    finetuned_tagged_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        CONFIG['model_name'],\n",
        "        num_labels=CONFIG['num_labels'],\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    ).to(device)\n",
        "    \n",
        "    # Setup training arguments\n",
        "    tagged_output_dir = CONFIG['output_dir'] + '_tagged'\n",
        "    training_args_tagged = setup_training_args(tagged_output_dir)\n",
        "    \n",
        "    # Disable wandb reporting in training args\n",
        "    training_args_tagged.report_to = []\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer_tagged = MultiLabelTrainer(\n",
        "        model=finetuned_tagged_model,\n",
        "        args=training_args_tagged,\n",
        "        train_dataset=train_dataset_tagged,\n",
        "        eval_dataset=val_dataset_tagged,\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])]\n",
        "    )\n",
        "    \n",
        "    # Train the model with progress tracking\n",
        "    print(\"Training started...\")\n",
        "    \n",
        "    # Add progress callback for training\n",
        "    from transformers import TrainerCallback\n",
        "    import time\n",
        "    \n",
        "    class ProgressCallback(TrainerCallback):\n",
        "        def __init__(self):\n",
        "            self.start_time = time.time()\n",
        "        \n",
        "        def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "            print(f\"\\n=== Starting Epoch {state.epoch + 1}/{args.num_train_epochs} ===\")\n",
        "        \n",
        "        def on_step_end(self, args, state, control, **kwargs):\n",
        "            if state.global_step % 50 == 0:  # Print every 50 steps\n",
        "                elapsed = time.time() - self.start_time\n",
        "                print(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('train_loss', 'N/A'):.4f} | \"\n",
        "                      f\"Time: {elapsed/60:.1f} min\")\n",
        "        \n",
        "        def on_evaluate(self, args, state, control, **kwargs):\n",
        "            if state.log_history:\n",
        "                last_log = state.log_history[-1]\n",
        "                eval_loss = last_log.get('eval_loss', 'N/A')\n",
        "                print(f\"Evaluation at step {state.global_step}: Eval Loss = {eval_loss}\")\n",
        "    \n",
        "    # Add progress callback to trainer\n",
        "    trainer_tagged.add_callback(ProgressCallback())\n",
        "    \n",
        "    trainer_tagged.train()\n",
        "    \n",
        "    # Save the model\n",
        "    trainer_tagged.save_model()\n",
        "    tokenizer.save_pretrained(tagged_output_dir)\n",
        "    \n",
        "    print(\"Fine-tuning with tagging completed!\")\n",
        "    print(f\"Model saved to: {tagged_output_dir}\")\n",
        "    \n",
        "    # Log model safely\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(finetuned_tagged_model, \"model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model to MLflow: {e}\")\n",
        "\n",
        "# Evaluate fine-tuned tagged model\n",
        "print(\"\\nEvaluating fine-tuned tagged model...\")\n",
        "tagged_predictions, tagged_labels = evaluate_model(\n",
        "    finetuned_tagged_model, test_dataset_tagged, batch_size=CONFIG['batch_size'], device=device\n",
        ")\n",
        "\n",
        "tagged_metrics = compute_metrics(tagged_predictions, tagged_labels, LABELS)\n",
        "\n",
        "print(\"\\n=== FINE-TUNED TAGGED MODEL RESULTS ===\")\n",
        "print(f\"Mean AUC: {tagged_metrics['mean_auc']:.4f}\")\n",
        "print(f\"Neutral Accuracy: {tagged_metrics.get('neutral_accuracy', 0):.4f}\")\n",
        "print(f\"Neutral FP Rate: {tagged_metrics.get('neutral_fp_rate', 0):.4f}\")\n",
        "print(\"\\nPer-label AUC:\")\n",
        "for label in LABELS:\n",
        "    print(f\"  {label}: {tagged_metrics[f'auc_{label}']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison_analysis",
      "metadata": {},
      "source": [
        "## Comprehensive Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model_comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison\n",
        "print(\"Creating comprehensive model comparison...\")\n",
        "\n",
        "# Compile all metrics\n",
        "all_metrics = {\n",
        "    'Baseline': baseline_metrics,\n",
        "    'Fine-tuned': finetuned_metrics,\n",
        "    'Fine-tuned + Tagged': tagged_metrics\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for model_name, metrics in all_metrics.items():\n",
        "    row = {'Model': model_name}\n",
        "    row['Mean_AUC'] = metrics['mean_auc']\n",
        "    row['Neutral_Accuracy'] = metrics.get('neutral_accuracy', 0)\n",
        "    row['Neutral_FP_Rate'] = metrics.get('neutral_fp_rate', 0)\n",
        "    \n",
        "    # Add per-label AUCs\n",
        "    for label in LABELS:\n",
        "        row[f'AUC_{label}'] = metrics[f'auc_{label}']\n",
        "    \n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Display comparison table\n",
        "print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
        "print(comparison_df[['Model', 'Mean_AUC', 'Neutral_Accuracy', 'Neutral_FP_Rate']].to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "print(\"\\n=== PER-LABEL AUC COMPARISON ===\")\n",
        "auc_columns = ['Model'] + [f'AUC_{label}' for label in LABELS]\n",
        "print(comparison_df[auc_columns].to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\n=== PERFORMANCE IMPROVEMENTS ===\")\n",
        "baseline_auc = comparison_df.iloc[0]['Mean_AUC']\n",
        "finetuned_auc = comparison_df.iloc[1]['Mean_AUC']\n",
        "tagged_auc = comparison_df.iloc[2]['Mean_AUC']\n",
        "\n",
        "finetuned_improvement = ((finetuned_auc - baseline_auc) / baseline_auc) * 100\n",
        "tagged_improvement = ((tagged_auc - baseline_auc) / baseline_auc) * 100\n",
        "\n",
        "print(f\"Fine-tuned vs Baseline: {finetuned_improvement:+.2f}%\")\n",
        "print(f\"Fine-tuned+Tagged vs Baseline: {tagged_improvement:+.2f}%\")\n",
        "print(f\"Tagged vs Fine-tuned: {((tagged_auc - finetuned_auc) / finetuned_auc) * 100:+.2f}%\")\n",
        "\n",
        "# Save comparison results\n",
        "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"\\nComparison results saved to 'model_comparison_results.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualization",
      "metadata": {},
      "source": [
        "## Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_visualizations",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Mean AUC Comparison\n",
        "ax1 = axes[0, 0]\n",
        "models = comparison_df['Model']\n",
        "mean_aucs = comparison_df['Mean_AUC']\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "bars1 = ax1.bar(models, mean_aucs, color=colors, alpha=0.8)\n",
        "ax1.set_title('Mean AUC Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Mean AUC Score')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars1, mean_aucs):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Per-Label AUC Heatmap\n",
        "ax2 = axes[0, 1]\n",
        "auc_data = comparison_df[[f'AUC_{label}' for label in LABELS]].values\n",
        "im = ax2.imshow(auc_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
        "ax2.set_title('Per-Label AUC Heatmap', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(range(len(LABELS)))\n",
        "ax2.set_xticklabels(LABELS, rotation=45, ha='right')\n",
        "ax2.set_yticks(range(len(models)))\n",
        "ax2.set_yticklabels(models)\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(models)):\n",
        "    for j in range(len(LABELS)):\n",
        "        text = ax2.text(j, i, f'{auc_data[i, j]:.3f}', \n",
        "                       ha='center', va='center', color='black', fontweight='bold')\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax2, shrink=0.8)\n",
        "cbar.set_label('AUC Score')\n",
        "\n",
        "# 3. Neutral Classification Performance\n",
        "ax3 = axes[1, 0]\n",
        "neutral_acc = comparison_df['Neutral_Accuracy']\n",
        "neutral_fp = comparison_df['Neutral_FP_Rate']\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "bars3a = ax3.bar(x - width/2, neutral_acc, width, label='Neutral Accuracy', \n",
        "                 color='green', alpha=0.7)\n",
        "bars3b = ax3.bar(x + width/2, neutral_fp, width, label='Neutral FP Rate', \n",
        "                 color='red', alpha=0.7)\n",
        "\n",
        "ax3.set_title('Neutral Classification Performance', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Score')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(models)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim(0, 1)\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars3a, bars3b]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 4. Improvement Analysis\n",
        "ax4 = axes[1, 1]\n",
        "improvements = [\n",
        "    0,  # Baseline (reference)\n",
        "    finetuned_improvement,\n",
        "    tagged_improvement\n",
        "]\n",
        "\n",
        "colors_imp = ['gray', 'orange' if finetuned_improvement >= 0 else 'red', \n",
        "              'green' if tagged_improvement >= 0 else 'red']\n",
        "\n",
        "bars4 = ax4.bar(models, improvements, color=colors_imp, alpha=0.8)\n",
        "ax4.set_title('Mean AUC Improvement vs Baseline', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('Improvement (%)')\n",
        "ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar, value in zip(bars4, improvements):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, \n",
        "             bar.get_height() + (0.1 if value >= 0 else -0.2),\n",
        "             f'{value:+.2f}%', ha='center', \n",
        "             va='bottom' if value >= 0 else 'top', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('toxic_bert_comparison_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization saved as 'toxic_bert_comparison_results.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusions",
      "metadata": {},
      "source": [
        "## Conclusions and Future Work\n",
        "\n",
        "### Summary\n",
        "This notebook successfully implemented and compared three approaches to toxic comment classification:\n",
        "\n",
        "1. **Baseline Toxic-BERT**: Pre-trained model without modifications\n",
        "2. **Fine-tuned Toxic-BERT**: Model fine-tuned on balanced dataset\n",
        "3. **Fine-tuned + Tagged**: Fine-tuned model with subtle toxicity tagging\n",
        "\n",
        "### Key Findings\n",
        "- **Class Balancing**: Created balanced dataset with equal representation across toxicity categories\n",
        "- **Fine-tuning Impact**: Evaluated whether domain-specific fine-tuning improves performance\n",
        "- **Tagging Effectiveness**: Tested if subtle toxicity tagging provides additional benefits\n",
        "- **Neutral Classification**: Maintained high accuracy for non-toxic content across all models\n",
        "\n",
        "### Technical Contributions\n",
        "- **Balanced Dataset Creation**: Systematic approach to address class imbalance\n",
        "- **Subtle Toxicity Tagging**: Novel tagging method for implicit toxic patterns\n",
        "- **Comprehensive Evaluation**: Multi-metric comparison framework\n",
        "- **MLflow Integration**: Experiment tracking and reproducibility\n",
        "\n",
        "### Future Work\n",
        "1. **Extended Tagging Methods**: Implement additional tagging strategies from previous research\n",
        "2. **Cross-Dataset Validation**: Test generalization on other toxicity datasets\n",
        "3. **Computational Analysis**: Compare training time and inference costs\n",
        "4. **Ensemble Methods**: Combine multiple approaches for improved performance\n",
        "5. **Real-world Deployment**: Test models in production environments\n",
        "\n",
        "### Usage Instructions\n",
        "1. Ensure you have the Jigsaw Toxic Comment dataset in the correct directory structure\n",
        "2. Install required dependencies: `transformers`, `torch`, `sklearn`, `mlflow`\n",
        "3. Run cells sequentially for complete comparison\n",
        "4. Adjust `CONFIG` parameters for different experimental settings\n",
        "5. Monitor GPU memory usage during training\n",
        "\n",
        "### Model Outputs\n",
        "- **Trained Models**: Saved in `./models/` directory\n",
        "- **Results**: `model_comparison_results.csv`\n",
        "- **Visualizations**: `toxic_bert_comparison_results.png`\n",
        "- **MLflow Logs**: Experiment tracking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export_results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export comprehensive results\n",
        "def export_comprehensive_results():\n",
        "    \"\"\"Export all results to files.\"\"\"\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Create results dictionary\n",
        "    results = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'config': CONFIG,\n",
        "            'labels': LABELS,\n",
        "            'device': str(device)\n",
        "        },\n",
        "        'metrics': {\n",
        "            'baseline': baseline_metrics,\n",
        "            'finetuned': finetuned_metrics,\n",
        "            'tagged': tagged_metrics\n",
        "        },\n",
        "        'comparison': comparison_df.to_dict('records')\n",
        "    }\n",
        "    \n",
        "    # Save to JSON\n",
        "    with open('toxic_bert_experiment_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "    \n",
        "    # Save detailed comparison\n",
        "    comparison_df.to_csv('detailed_model_comparison.csv', index=False)\n",
        "    \n",
        "    # Create summary report\n",
        "    summary_report = f\"\"\"\n",
        "# Toxic-BERT Fine-Tuning Experiment Results\n",
        "\n",
        "## Experiment Configuration\n",
        "- Model: {CONFIG['model_name']}\n",
        "- Samples per class: {CONFIG['samples_per_class']:,}\n",
        "- Training epochs: {CONFIG['num_epochs']}\n",
        "- Batch size: {CONFIG['batch_size']}\n",
        "- Learning rate: {CONFIG['learning_rate']}\n",
        "\n",
        "## Overall Results\n",
        "| Model | Mean AUC | Neutral Accuracy | Neutral FP Rate |\n",
        "|-------|----------|------------------|------------------|\n",
        "\"\"\"\n",
        "    \n",
        "    for _, row in comparison_df.iterrows():\n",
        "        summary_report += f\"| {row['Model']} | {row['Mean_AUC']:.4f} | {row['Neutral_Accuracy']:.4f} | {row['Neutral_FP_Rate']:.4f} |\\n\"\n",
        "    \n",
        "    # Add per-label results\n",
        "    summary_report += \"\\n## Per-Label AUC Results\\n\"\n",
        "    summary_report += \"| Label | Baseline | Fine-tuned | FT+Tagged | Best |\\n\"\n",
        "    summary_report += \"|-------|----------|------------|-----------|------|\\n\"\n",
        "    \n",
        "    for label in LABELS:\n",
        "        baseline_val = comparison_df.iloc[0][f'AUC_{label}']\n",
        "        finetuned_val = comparison_df.iloc[1][f'AUC_{label}']\n",
        "        tagged_val = comparison_df.iloc[2][f'AUC_{label}']\n",
        "        best_model = ['Baseline', 'Fine-tuned', 'FT+Tagged'][np.argmax([baseline_val, finetuned_val, tagged_val])]\n",
        "        \n",
        "        summary_report += f\"| {label} | {baseline_val:.4f} | {finetuned_val:.4f} | {tagged_val:.4f} | {best_model} |\\n\"\n",
        "    \n",
        "    # Save summary\n",
        "    with open('experiment_summary.md', 'w') as f:\n",
        "        f.write(summary_report)\n",
        "    \n",
        "    print(\"Results exported successfully!\")\n",
        "    print(\"Files created:\")\n",
        "    print(\"  - toxic_bert_experiment_results.json\")\n",
        "    print(\"  - detailed_model_comparison.csv\")\n",
        "    print(\"  - experiment_summary.md\")\n",
        "    print(\"  - toxic_bert_comparison_results.png\")\n",
        "\n",
        "# Uncomment to export results\n",
        "# export_comprehensive_results()\n",
        "\n",
        "print(\"Export functions loaded successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "toxic_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
